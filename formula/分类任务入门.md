# 2.6
## 1 深度学习
- 本质：
    - 输入：一堆特征（数字），embedding
    - 输出与 label 做 loss，训练学习

>线性回归
- 房价预测：
    - 输入（特征）：年份、面积、楼层...
    - 标签：房价

>逻辑回归
- 葡萄酒分类
    - 输入（特征）：一堆数字
    - 标签：品级
- 鸢尾花分类
    - 输入（特征）：一堆数字
    - 标签：鸢尾花的种类

## 2 NLP

>清华新闻数据集，文本分类
- 输入：一个新闻标题
    - Embedding: Onehot编码 -> 随机向量 -> nn.Embedding -> 是否能自己训练这个 Embedding（用别人训练好的 Embedding）
- 标签：不同的新闻类别（一共十个）

## 3 Word2Vec
- 动机：想要训练一个 `词向量`, 能够对每个词进行编码
- 重要假设：词的距离越近，越相关
- 流程：
    1. 拿到一堆的数据（语料），课上例子：数学原始数据.csv; 各种小说、文章
    2. 构建一个词表（字典）vocab, 用来索引 Embedding，拿到每个词的向量表示
    3. 构建数据集（Skip-gram）
        - win_size: 中心词两边的语境词（上下文词）
        - 负采样
            - 随机
            - 基于词频的负采样









